{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "from scipy import ndimage\n",
    "from datetime import datetime\n",
    "# from scipy.ndimage.interpolation import rotate\n",
    "# from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "import tools\n",
    "import structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PrePare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare variables\n",
    "V = tools.Vars()\n",
    "# V.put('data_dir', os.getcwd() + \"/../full_data_cell/\")\n",
    "# V.put('data_dir', os.getcwd() + \"/../full_data_cell_bgrm/\")\n",
    "V.put('data_dir', \"/root/data/cell_unscale\")\n",
    "V.put('cell_xy_file', \"/root/data/cell_unscale/cell_xy.p\")\n",
    "V.put('cell_x', 140)\n",
    "V.put('cell_y', 280)\n",
    "V.put('size_re', 128)\n",
    "V.put('test_set_size', 4096)\n",
    "V.put('batch_size', 64)\n",
    "V.put('learning_rate_init', 0.003)\n",
    "V.put('lr_change_std', 4000)\n",
    "V.put('training_iters', 200000)\n",
    "V.put('display_step', 100)\n",
    "V.put('save_step', 3000)\n",
    "V.put('do_bn', True)\n",
    "V.put('do_l2', False)\n",
    "V.put('l2_rate', 0.00003)\n",
    "V.put('boardDir', \"./board\")\n",
    "V.put('expTag', \"CompactCNN\")\n",
    "V.put('expName', datetime.today().strftime(\"%Y%m%d_%H%M_\") + \"16.32.64.128.512.512_e003heu_CWAtt\")\n",
    "V.put('model_save_path', \"./model/{}/{}\".format(V.get('expTag'), V.get('expName')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Definition Done\n"
     ]
    }
   ],
   "source": [
    "def cord2grid(V, cord) :\n",
    "    slc_num_x = V.get('slc_num_x')\n",
    "    slc_num_y = V.get('slc_num_y')\n",
    "    cord_flatten = cord[:,1]*slc_num_x + cord[:,0]\n",
    "    cord_hot = tf.one_hot(cord_flatten, slc_num_x*slc_num_y, name=\"cord_hot\")\n",
    "#     cord_hot1 = tf.one_hot(cord[:,0], slc_num_y)\n",
    "#     cord_hot2 = tf.one_hot(cord[:,1], slc_num_x)\n",
    "#     cord_hot = tf.concat([cord_hot1, cord_hot2], 1)\n",
    "#     cord_hot = tf.reshape(cord_hot, [-1, slc_num_y, slc_num_x], name=\"cord_hot\")\n",
    "    return tf.cast(cord_hot, tf.float32)\n",
    "\n",
    "# def partition_by_class(V, fList, cList, xyList, cDic) :\n",
    "# #     test_set_size = V.get('test_set_size')\n",
    "# #     train_set_size = len(fList) - test_set_size\n",
    "# #     V.put(\"train_set_size\", train_set_size)\n",
    "#     partitions = np.random.permutation(len(fList))\n",
    "#     files_error1 = [ [] for i in range(V.get('n_output')) ]\n",
    "#     labels_error1 = [ [] for i in range(V.get('n_output')) ]\n",
    "#     files_errorn = [ [] for i in range(V.get('n_output')) ]\n",
    "#     labels_errorn = [ [] for i in range(V.get('n_output')) ]\n",
    "# #     test_files_errorn = [ [] for i in range(V.get('n_output')) ]\n",
    "# #     test_labels_errorn = [ [] for i in range(V.get('n_output')) ]\n",
    "# #     test_xys = []\n",
    "#     for i in range(V.get('n_output')) :\n",
    "#         for p in partitions :\n",
    "#             if cList[p] == i :\n",
    "#                 if len(fList[p]) == 1 :\n",
    "#                     files_error1[i].append(fList[p])\n",
    "#                     labels_error1[i].append(cList[p])\n",
    "#                 elif len(fList[p]) >= 2 :\n",
    "#                     files_errorn[i].append(fList[p])\n",
    "#                     labels_errorn[i].append(cList[p])\n",
    "# #                 elif len(fList[p]) > 2 :\n",
    "# #                     test_files_errorn[i].append(fList[p])\n",
    "# #                     test_labels_errorn[i].append(cList[p])\n",
    "#                 else :\n",
    "#                     print len(fList[p])\n",
    "#     for i in range(len(files_error1)) :\n",
    "#         print len(files_error1[i]), len(files_errorn[i])\n",
    "#     print \"Separation by error done\"\n",
    "    \n",
    "#     train_f_list = []\n",
    "#     train_l_list = []\n",
    "#     test_files_error1 = [ [] for i in range(V.get('n_output')) ]\n",
    "#     test_labels_error1 = [ [] for i in range(V.get('n_output')) ]\n",
    "#     test_files_errorn = [ [] for i in range(V.get('n_output')) ]\n",
    "#     test_labels_errorn = [ [] for i in range(V.get('n_output')) ]\n",
    "#     max_n = 500\n",
    "#     for i in range(V.get('n_output')) :\n",
    "#         if len(files_error1[i]) > max_n :\n",
    "#             m = 100\n",
    "#         else :\n",
    "#             m = len(files_error1[i])/5\n",
    "#         test_files_error1[i].extend(files_error1[i][:m])\n",
    "#         test_labels_error1[i].extend(labels_error1[i][:m])\n",
    "#         for j in range(len(files_error1[i])-m) :\n",
    "#             train_f_list.extend(files_error1[i][m+j])\n",
    "#             train_l_list.extend([ labels_error1[i][m+j] for k in range(len(files_error1[i][m+j]))])\n",
    "        \n",
    "#         if len(files_errorn[i]) > max_n :\n",
    "#             m = 100\n",
    "#         else :\n",
    "#             m = len(files_errorn[i])/5\n",
    "#         test_files_errorn[i].extend(files_errorn[i][:m])\n",
    "#         test_labels_errorn[i].extend(labels_errorn[i][:m])\n",
    "#         for j in range(len(files_errorn[i])-m) :\n",
    "#             train_f_list.extend(files_errorn[i][m+j])\n",
    "#             train_l_list.extend([ labels_errorn[i][m+j] for k in range(len(files_errorn[i][m+j]))])\n",
    "        \n",
    "# #     train_f_list = list(itertools.chain.from_iterable(train_f_list))\n",
    "    \n",
    "#     test_images_error1 = [ [] for i in range(V.get('n_output')) ]\n",
    "#     test_images_errorn = [ [] for i in range(V.get('n_output')) ]\n",
    "#     for i in range(V.get('n_output')) :\n",
    "#         for fs in test_files_error1[i] :\n",
    "#             imgs = []\n",
    "#             for f in fs :\n",
    "#                 imgs.append(ndimage.imread(f))\n",
    "#             test_images_error1[i].append(np.array(imgs))\n",
    "#         for fs in test_files_errorn[i] :\n",
    "#             imgs = []\n",
    "#             for f in fs :\n",
    "#                 imgs.append(ndimage.imread(f))\n",
    "#             test_images_errorn[i].append(np.array(imgs))\n",
    "    \n",
    "#     test_images = []\n",
    "#     test_labels = []\n",
    "#     for i in range(V.get('n_output')) :\n",
    "#         test_images.extend(test_images_error1[i])\n",
    "#         test_labels.extend(test_labels_error1[i])\n",
    "#         test_images.extend(test_images_errorn[i])\n",
    "#         test_labels.extend(test_labels_errorn[i])\n",
    "        \n",
    "#     V.put('test_set_size', len(test_images))\n",
    "#     print \"Partition done\"\n",
    "#     return [train_f_list, train_l_list, test_images, test_labels, \n",
    "#             test_images_error1, test_labels_error1, test_images_errorn, test_labels_errorn]\n",
    "\n",
    "def current_model(V, x, y, cord, phase, keep_prob, learning_rate) :\n",
    "    n_c1 = 16\n",
    "    n_c2 = 32\n",
    "    n_c3 = 64\n",
    "    n_c4 = 128\n",
    "    n_fc1 = 512\n",
    "    n_fc2 = 512\n",
    "    n_output = V.get('n_output')\n",
    "\n",
    "    with tf.name_scope(\"Weight\") :\n",
    "        W = {\"c1_1\" : tf.Variable(tf.truncated_normal([3, 3, 3, n_c1], stddev=0.1), name=\"w_c1_1\"),\n",
    "            \"c1_2\" : tf.Variable(tf.truncated_normal([3, 3, n_c1, n_c1], stddev=0.1), name=\"w_c1_2\"),\n",
    "            \"c2_1\" : tf.Variable(tf.truncated_normal([3, 3, n_c1, n_c2], stddev=0.1), name=\"w_c2_1\"),\n",
    "            \"c2_2\" : tf.Variable(tf.truncated_normal([3, 3, n_c2, n_c2], stddev=0.1), name=\"w_c2_2\"),\n",
    "            \"c3_1\" : tf.Variable(tf.truncated_normal([3, 3, n_c2, n_c3], stddev=0.1), name=\"w_c3_1\"),\n",
    "            \"c3_2\" : tf.Variable(tf.truncated_normal([3, 3, n_c3, n_c3], stddev=0.1), name=\"w_c3_2\"),\n",
    "            \"c4_1\" : tf.Variable(tf.truncated_normal([3, 3, n_c3, n_c4], stddev=0.1), name=\"w_c4_1\"),\n",
    "            \"c4_2\" : tf.Variable(tf.truncated_normal([3, 3, n_c4, n_c4], stddev=0.1), name=\"w_c4_2\"),\n",
    "            \"fc1\" : tf.Variable(tf.truncated_normal([8*8*n_c4, n_fc1], stddev=0.1), name=\"w_fc1\"),\n",
    "            \"fc2\" : tf.Variable(tf.truncated_normal([n_fc1, n_fc2], stddev=0.1), name=\"w_fc2\"),\n",
    "            \"out\" : tf.Variable(tf.truncated_normal([n_fc2, n_output], stddev=0.1), name=\"w_out\")}\n",
    "\n",
    "    with tf.name_scope(\"Bias\") :\n",
    "        B = {\"c1_1\" : tf.Variable(tf.random_normal([n_c1], stddev=0.1), name=\"b_c1_1\"),\n",
    "            \"c1_2\" : tf.Variable(tf.random_normal([n_c1], stddev=0.1), name=\"b_c1_2\"),\n",
    "            \"c2_1\" : tf.Variable(tf.random_normal([n_c2], stddev=0.1), name=\"b_c2_1\"),\n",
    "            \"c2_2\" : tf.Variable(tf.random_normal([n_c2], stddev=0.1), name=\"b_c2_2\"),\n",
    "            \"c3_1\" : tf.Variable(tf.random_normal([n_c3], stddev=0.1), name=\"b_c3_1\"),\n",
    "            \"c3_2\" : tf.Variable(tf.random_normal([n_c3], stddev=0.1), name=\"b_c3_2\"),\n",
    "            \"c4_1\" : tf.Variable(tf.random_normal([n_c4], stddev=0.1), name=\"b_c4_1\"),\n",
    "            \"c4_2\" : tf.Variable(tf.random_normal([n_c4], stddev=0.1), name=\"b_c4_2\"),\n",
    "            \"fc1\" : tf.Variable(tf.random_normal([n_fc1], stddev=0.1), name=\"b_fc1\"),\n",
    "            \"fc2\" : tf.Variable(tf.random_normal([n_fc2], stddev=0.1), name=\"b_fc2\"),\n",
    "            \"out\" : tf.Variable(tf.random_normal([n_output], stddev=0.1), name=\"b_out\")}\n",
    "        \n",
    "    with tf.name_scope(\"Weigth_coord\") :\n",
    "        W_c = {\"fc1\" : tf.Variable(tf.truncated_normal([7*4, n_fc2*n_output], stddev=0.1), name=\"w_fc1\")}\n",
    "    with tf.name_scope(\"Bias_coord\") :\n",
    "        B_c = {\"fc1\" : tf.Variable(tf.random_normal([n_fc2*n_output], stddev=0.1), name=\"b_fc1\")}\n",
    "\n",
    "#     pool1 = structure.conv33pool2(x, W[\"c1_1\"], B[\"c1_1\"], W[\"c1_2\"], B[\"c1_2\"], phase, keep_prob,\n",
    "#                                   \"conv1_1\", \"conv1_2\", \"pool1\", \"Conv1\",\n",
    "#                                  do_bn=V.get('do_bn'))\n",
    "#     pool2 = structure.conv33pool2(pool1, W[\"c2_1\"], B[\"c2_1\"], W[\"c2_2\"], B[\"c2_2\"], phase, keep_prob,\n",
    "#                                   \"conv2_1\", \"conv2_2\", \"pool2\", \"Conv2\",\n",
    "#                                  do_bn=V.get('do_bn'))\n",
    "#     pool3 = structure.conv33pool2(pool2, W[\"c3_1\"], B[\"c3_1\"], W[\"c3_2\"], B[\"c3_2\"], phase, keep_prob, \n",
    "#                                   \"conv3_1\", \"conv3_2\", \"pool3\", \"Conv3\",\n",
    "#                                  do_bn=V.get('do_bn'))\n",
    "#     pool4 = structure.conv33pool2(pool3, W[\"c4_1\"], B[\"c4_1\"], W[\"c4_2\"], B[\"c4_2\"], phase, keep_prob, \n",
    "#                                   \"conv4_1\", \"conv4_2\", \"pool4\", \"Conv4\",\n",
    "#                                  do_bn=V.get('do_bn'))\n",
    "#     pool4 = tf.reshape(pool4, [-1, 8*8*n_c4])\n",
    "    \n",
    "    # conv 5 pool 2\n",
    "    W1 = [W[\"c1_1\"], W[\"c1_2\"]]\n",
    "    B1 = [B[\"c1_1\"], B[\"c1_2\"]]\n",
    "    S1 = [[1,1,1,1], [1,1,1,1], [1,2,2,1]]\n",
    "    N1 = [\"conv1_1\", \"conv1_2\", \"pool1\"]\n",
    "    pool1 = structure.conv_univ(x, W1, B1, S1, N1, phase, \"Conv1\", do_bn=V.get('do_bn'))\n",
    "    \n",
    "    # conv 5 pool 2\n",
    "    W2 = [W[\"c2_1\"], W[\"c2_2\"]]\n",
    "    B2 = [B[\"c2_1\"], B[\"c2_2\"]]\n",
    "    S2 = [[1,1,1,1], [1,1,1,1], [1,2,2,1]]\n",
    "    N2 = [\"conv2_1\", \"conv2_2\", \"pool2\"]\n",
    "    pool2 = structure.conv_univ(pool1, W2, B2, S2, N2, phase, \"Conv2\", do_bn=V.get('do_bn'))\n",
    "    \n",
    "    # conv 5 pool 2\n",
    "    W3 = [W[\"c3_1\"], W[\"c3_2\"]]\n",
    "    B3 = [B[\"c3_1\"], B[\"c3_2\"]]\n",
    "    S3 = [[1,1,1,1], [1,1,1,1], [1,2,2,1]]\n",
    "    N3 = [\"conv3_1\", \"conv3_2\", \"pool3\"]\n",
    "    pool3 = structure.conv_univ(pool2, W3, B3, S3, N3, phase, \"Conv3\", do_bn=V.get('do_bn'))\n",
    "    \n",
    "    # conv 5 pool 2\n",
    "    W4 = [W[\"c4_1\"], W[\"c4_2\"]]\n",
    "    B4 = [B[\"c4_1\"], B[\"c4_2\"]]\n",
    "    S4 = [[1,1,1,1], [1,1,1,1], [1,2,2,1]]\n",
    "    N4 = [\"conv4_1\", \"conv4_2\", \"pool4\"]\n",
    "    pool4 = structure.conv_univ(pool3, W4, B4, S4, N4, phase, \"Conv4\", do_bn=V.get('do_bn'))\n",
    "    \n",
    "    # reshape\n",
    "    pool4 = tf.reshape(pool4, [-1, 8*8*n_c4])\n",
    "\n",
    "    # fully connected\n",
    "    fc1 = structure.fc_univ(pool4, W[\"fc1\"], B[\"fc1\"], \"fc1\", phase, \"FC1\", do_bn=V.get('do_bn'))\n",
    "    fc2 = structure.fc_univ(fc1, W[\"fc2\"], B[\"fc2\"], \"fc2\", phase, \"FC2\", do_bn=V.get('do_bn'))\n",
    "    \n",
    "    with tf.name_scope(\"Coord\") :\n",
    "        fc_coord = tf.nn.bias_add(tf.matmul(cord, W_c[\"fc1\"]), B_c[\"fc1\"])\n",
    "        fc_coord = tf.layers.batch_normalization(fc_coord, axis=1, center=True, scale=True, training=phase)\n",
    "        fc_coord = tf.nn.relu(fc_coord, name=\"fc_coord\")\n",
    "        tf.summary.histogram(\"weights\", W_c[\"fc1\"])\n",
    "        tf.summary.histogram(\"bias\", B_c[\"fc1\"])\n",
    "        tf.summary.histogram(\"activations\", fc_coord)\n",
    "        fc_coord = tf.reshape(fc_coord, [-1, n_fc2, n_output])\n",
    "\n",
    "    with tf.name_scope(\"OUT\") :\n",
    "        W_tiled = tf.tile(tf.expand_dims(W[\"out\"],0), [tf.shape(x)[0],1,1], name=\"W_tiled\")\n",
    "        W_att = tf.multiply(W_tiled, fc_coord, name=\"W_att\")\n",
    "        fc2_tiled = tf.tile(tf.expand_dims(fc2,2), [1,1,n_output], name=\"fc2_tiled\")\n",
    "        out = tf.reduce_sum(fc2_tiled*W_att, axis=1)\n",
    "        out = tf.nn.bias_add(out, B[\"out\"], name=\"out\")\n",
    "#         out = tf.nn.bias_add(tf.matmul(fc2, W[\"out\"]), B[\"out\"], name=\"out\")\n",
    "        tf.summary.histogram(\"weights\", W[\"out\"])\n",
    "        tf.summary.histogram(\"bias\", B[\"out\"])\n",
    "        tf.summary.histogram(\"activations\", out)\n",
    "\n",
    "    with tf.name_scope(\"Cost\") :\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=y), name=\"loss\")\n",
    "        if V.get('do_l2') :\n",
    "            loss = loss + V.get('l2_rate')*tf.add_n([ tf.nn.l2_loss(w) for w in W.values() ]) # L2 regularization\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops) :\n",
    "            opt = tf.train.AdamOptimizer(learning_rate=learning_rate, name=\"adam\").minimize(loss, name=\"opt\")\n",
    "\n",
    "    with tf.name_scope(\"Accuracy\") :\n",
    "        corPred = tf.equal(tf.argmax(out, 1), tf.argmax(y, 1), name=\"corPred\")\n",
    "        acc = tf.reduce_mean(tf.cast(corPred, tf.float32), name=\"acc\")\n",
    "\n",
    "    with tf.name_scope(\"AvgPred\") :\n",
    "        intm = tf.reduce_mean(out, axis=0)\n",
    "        sm = tf.nn.softmax(out, name=\"sm\")\n",
    "        sm_intm = tf.reduce_mean(sm, axis=0, name=\"sm_intm\")\n",
    "        pred_test = tf.argmax(sm_intm)\n",
    "        loss_avg = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=intm, labels=y), name=\"loss_avg\")\n",
    "        acc_avg = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(sm_intm), tf.argmax(y, 1)), tf.float32), name=\"acc_avg\")\n",
    "\n",
    "    print \"Structure build Complete\"\n",
    "    return [loss, opt, corPred, acc, pred_test, loss_avg, acc_avg]\n",
    "\n",
    "print \"Function Definition Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Created\n"
     ]
    }
   ],
   "source": [
    "# Create session\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)))\n",
    "print \"Session Created\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes :\n",
      "['ACT_HABU_PTN_ER', 'BLACK_IMUL', 'BLACK_SPOT', 'CNT_FILM', 'CNT_NOT_OPEN', 'CNT_NOT_OPEN_S', 'CORROSION', 'GAT1_HABU_PTN_E', 'GAT2_HABU_PTN_E', 'HOLE_PTN_ERR', 'ILD_FLAKE', 'LARVA', 'PAST_REPAIR', 'PI_UNDERLAYER', 'PI_UNDER_K', 'PI_UNDER_S', 'PTN_ERR', 'SD_BOMB', 'SD_SPOT', 'SHORT_D', 'SHORT_P', 'TI_YUSIL']\n",
      "# of class : 22\n",
      "# of images : 31759\n",
      "# of cells : 44395\n",
      "Loading Done\n",
      "# of train cells : 38452\n",
      "Train partition done\n",
      "Test partition done\n",
      "Batching done\n"
     ]
    }
   ],
   "source": [
    "# Prepare\n",
    "rm_list = [\"PASS\", \"NORMAL\"]\n",
    "x_grid_std = [0, 38, 77, 100, 128]\n",
    "y_grid_std = [0, 18, 36, 54, 77, 93, 109, 128]\n",
    "V.put('slc_num_x', len(x_grid_std)-1)\n",
    "V.put('slc_num_y', len(y_grid_std)-1)\n",
    "fList, cList, xyList, cDic = tools.load_data_cell(V, rm_list, x_grid_std, y_grid_std)\n",
    "[train_f_list, train_l_list, train_xy_list, \n",
    " test_images, test_labels, test_xys] = tools.partition(V, fList, cList, xyList, cDic)\n",
    "# [train_f_list, train_l_list, test_images, test_labels,\n",
    "#  test_images_error1, test_labels_error1, \n",
    "#  test_images_errorn, test_labels_errorn] = partition_by_class(V, fList, cList, xyList, cDic)\n",
    "\n",
    "# train_f_list, train_l_list = undersampling(train_f_list, train_l_list, cDic)\n",
    "# train_f_list, train_l_list = oversampling(train_f_list, train_l_list, cDic)\n",
    "# train_xy_list = [[]]*len(train_f_list)\n",
    "train_image_batch, train_label_batch, train_xy_batch = tools.pipeline_train(V, train_f_list, train_l_list, train_xy_list)\n",
    "\n",
    "# Placeholder\n",
    "x = tf.placeholder(tf.float32, [None, V.get('cell_y'), V.get('cell_x'), 3], name=\"x\")\n",
    "y = tf.placeholder(tf.uint8, [None], name=\"y\")\n",
    "y_hot = tf.one_hot(y, V.get('n_output'), name=\"y_hot\")\n",
    "cord = tf.placeholder(tf.int32, [None, 2], name=\"cord\")\n",
    "cord_grid = cord2grid(V, cord)\n",
    "phase = tf.placeholder(tf.bool, name=\"phase\")\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "prediction = tf.placeholder(tf.uint8, [None], name=\"prediction\")\n",
    "conf = tf.confusion_matrix(y, prediction, num_classes=V.get(\"n_output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def channel(x, a) :\n",
    "    if x < -a :\n",
    "        return 0\n",
    "    elif -a <= x < 0 :\n",
    "        return x+a\n",
    "    elif 0 <= x < a :\n",
    "        return -x+a\n",
    "    else :\n",
    "        return 0\n",
    "    \n",
    "np_channel = np.vectorize(channel)\n",
    "img = np.squeeze(test_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 140, 3)\n"
     ]
    }
   ],
   "source": [
    "print img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = 1\n",
    "w2 = 1\n",
    "w3 = 1\n",
    "b = -100\n",
    "a = 30\n",
    "out = np.zeros((278,138))\n",
    "for i in range(278) :\n",
    "    for j in range(138) :\n",
    "        img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure build Complete\n"
     ]
    }
   ],
   "source": [
    "# Build \n",
    "x_resize = tools.resize(V, x)\n",
    "x_std = tools.zero_channel(V, x_resize)\n",
    "[loss, opt, corPred, acc, pred_test, loss_avg, acc_avg] = current_model(V, x_std, y_hot, cord_grid,\n",
    "                                                                      phase, keep_prob, learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
