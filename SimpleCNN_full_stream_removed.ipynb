{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Import "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "# from scipy.ndimage.interpolation import rotate\n",
      "# from PIL import Image\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Utilities"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "class Vars() :\n",
      "    def __init__(self) :\n",
      "        self.handler = {}\n",
      "        \n",
      "    def get(self, key) :\n",
      "        return self.handler[key]\n",
      "    \n",
      "    def put(self, key, value) :\n",
      "        self.handler[key] = value"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def load_image(name) :\n",
      "    return ndimage.imread(var.get('dataDir') + cat + \"/\" + name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "PrePare"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Prepare variables\n",
      "V = Vars()\n",
      "V.put('data_dir', os.getcwd() + \"/../full_data_png/\")\n",
      "V.put('image_x', 256)\n",
      "V.put('image_y', 256)\n",
      "V.put('test_set_size', 4096)\n",
      "V.put('batch_size', 64)\n",
      "V.put('learning_rate', 0.003)\n",
      "V.put('training_iters', 400000)\n",
      "V.put('display_step', 100)\n",
      "V.put('save_step', 3000)\n",
      "V.put('do_l2', False)\n",
      "V.put('l2_rate', 0.0003)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load Data\n",
      "cats = os.listdir(V.get('data_dir'))\n",
      "# cats.remove(\"others\")\n",
      "# cats.remove(\"shchoi\")\n",
      "# cats.remove(\".ipynb_checkpoints\")\n",
      "cats.remove(\"PASS\")\n",
      "print cats\n",
      "fList = []\n",
      "cList = []\n",
      "cDic = { cats[i] : i for i in range(len(cats)) }\n",
      "for c in cats :\n",
      "    fCat = os.listdir(V.get('data_dir') + c)\n",
      "    for f in fCat :\n",
      "        if f.split('.')[-1] == \"png\" : #and f.split('_')[-2] == \"O\" :\n",
      "            fList.append(V.get('data_dir') + c + \"/\" + f)\n",
      "            cList.append(cDic[c])\n",
      "V.put('n_output', len(cDic))\n",
      "print \"Loading Done\""
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# visualize\n",
      "fig, ax = plt.subplots(2, 3, figsize=(30, 30))\n",
      "idxs = np.random.permutation(len(X_data))[:6]\n",
      "for i in range(2):\n",
      "    for j in range(3):\n",
      "        n = 2*i + j\n",
      "        x = X_data[idxs[n]]\n",
      "        print y_data[idxs[n]]\n",
      "        ax[i,j].imshow(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.unique(cList)"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(fList)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Build Structure "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Some Structure Function\n",
      "\n",
      "def conv3_bn(layer_in, w, b, phase, momentum=0.99) :\n",
      "    conv = tf.nn.conv2d(layer_in, w, strides=[1,1,1,1], padding='SAME')\n",
      "    conv = tf.nn.bias_add(conv, b)\n",
      "    bn = tf.layers.batch_normalization(conv, axis=3, center=True, scale=True, training=phase, momentum=momentum)\n",
      "    return tf.nn.relu(bn)\n",
      "\n",
      "def conv33pool2(layer_in, w1, b1, w2, b2, phase, name_scope) :\n",
      "    with tf.name_scope(name_scope) :\n",
      "        layer1 = conv3_bn(layer_in, w1, b1, phase)\n",
      "        layer2 = conv3_bn(layer1, w2, b2, phase)\n",
      "        pool = tf.nn.max_pool(layer2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
      "    return pool\n",
      "\n",
      "def conv333pool2(layer_in, w1, b1, w2, b2, phase, name_scope) :\n",
      "    with tf.name_scope(name_scope) :\n",
      "        layer1 = conv3_bn(layer_in, w1, b1, phase)\n",
      "        layer2 = conv3_bn(layer1, w2, b2, phase)\n",
      "        layer3 = conv3_bn(layer2, w3, b3, phase)\n",
      "        pool = tf.nn.max_pool(layer3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
      "    return pool\n",
      "\n",
      "def conv33pool4(layer_in, w1, b1, w2, b2, phase, name_scope) :\n",
      "    with tf.name_scope(name_scope) :\n",
      "        layer1 = conv3_bn(layer_in, w1, b1, phase)\n",
      "        layer2 = conv3_bn(layer1, w2, b2, phase)\n",
      "        pool = tf.nn.max_pool(layer2, ksize=[1,4,4,1], strides=[1,4,4,1], padding='SAME')\n",
      "    return pool\n",
      "\n",
      "def conv3333pool8(layer_in, w1, b1, w2, b2, phase, name_scope) :\n",
      "    with tf.name_scope(name_scope) :\n",
      "        layer1 = conv3_bn(layer_in, w1, b1, phase)\n",
      "        layer2 = conv3_bn(layer1, w2, b2, phase)\n",
      "        layer3 = conv3_bn(layer2, w3, b3, phase)\n",
      "        layer4 = conv3_bn(layer3, w4, b4, phase)\n",
      "        pool = tf.nn.max_pool(layer4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
      "    return pool"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build pipeline\n",
      "image_x = V.get('image_x')\n",
      "image_y = V.get('image_y')\n",
      "batch_size = V.get('batch_size')\n",
      "test_set_size = V.get('test_set_size')\n",
      "n_input = image_y*image_x\n",
      "n_output = V.get('n_output')\n",
      "\n",
      "all_images = tf.convert_to_tensor(fList, dtype=tf.string)\n",
      "all_labels = tf.convert_to_tensor(cList, dtype=tf.uint8)\n",
      "\n",
      "train_set_size = len(fList) - test_set_size\n",
      "partitions = [0] * len(fList)\n",
      "partitions[:test_set_size] = [1] * test_set_size\n",
      "np.random.shuffle(partitions)\n",
      "\n",
      "train_images, test_images = tf.dynamic_partition(all_images, partitions, 2)\n",
      "train_labels, test_labels = tf.dynamic_partition(all_labels, partitions, 2)\n",
      "\n",
      "train_input_queue = tf.train.slice_input_producer([train_images, train_labels], shuffle=True)\n",
      "test_input_queue = tf.train.slice_input_producer([test_images, test_labels], shuffle=False)\n",
      "\n",
      "file_content = tf.read_file(train_input_queue[0])\n",
      "train_image = tf.image.decode_png(file_content)\n",
      "train_label = train_input_queue[1]\n",
      "\n",
      "file_content = tf.read_file(test_input_queue[0])\n",
      "test_image = tf.image.decode_png(file_content)\n",
      "test_label = test_input_queue[1]\n",
      "\n",
      "train_image.set_shape([image_y, image_x, 3])\n",
      "test_image.set_shape([image_y, image_x, 3])\n",
      "\n",
      "train_image_batch, train_label_batch = tf.train.batch([train_image, train_label], batch_size = batch_size)\n",
      "test_image_batch, test_label_batch = tf.train.batch([test_image, test_label], batch_size = batch_size)\n",
      "\n",
      "# placeholder\n",
      "x = tf.placeholder(tf.float32, [None, image_y, image_x, 3])\n",
      "y = tf.placeholder(tf.uint8, [None])\n",
      "y_hot = tf.one_hot(y, n_output)\n",
      "phase = tf.placeholder(tf.bool)\n",
      "\n",
      "# preprocessing\n",
      "with tf.name_scope(\"Zero_Channel\") :\n",
      "    ch_mean = tf.reduce_mean(x, [1,2])\n",
      "    ch_mean = tf.expand_dims(ch_mean, 1)\n",
      "    ch_mean = tf.expand_dims(ch_mean, 2)\n",
      "    ch_mean = tf.tile(ch_mean, [1, image_y, image_x, 1])\n",
      "    x_std = x - ch_mean"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)))\n",
      "\n",
      "coord = tf.train.Coordinator()\n",
      "threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
      "\n",
      "sess.run(tf.global_variables_initializer())\n",
      "sess.run(tf.local_variables_initializer())\n",
      "\n",
      "x_batch, y_batch = sess.run([train_image_batch, train_label_batch])\n",
      "result = sess.run(x_std, feed_dict={x:x_batch})\n",
      "print plt.imshow(result[3].astype(np.uint8))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # 4 layer\n",
      "\n",
      "# n_out = np.max(np.unique(y_data)) + 1\n",
      "# n_in = X_data.shape[1]*X_data.shape[2]\n",
      "# n_ch1 = 16\n",
      "# n_ch2 = 32\n",
      "# n_ch3 = 128\n",
      "# W = {\"w1\" : tf.Variable(tf.truncated_normal([5, 5, 3, n_ch1], stddev=0.1)),\n",
      "#     \"w2\" : tf.Variable(tf.truncated_normal([5, 5, n_ch1, n_ch2], stddev=0.1)),\n",
      "#     \"w3\" : tf.Variable(tf.truncated_normal([14*14*n_ch2, n_ch3], stddev=0.1)),\n",
      "#     \"w4\" : tf.Variable(tf.truncated_normal([n_ch3, n_out], stddev=0.1))}\n",
      "# B = {\"b1\" : tf.Variable(tf.random_normal([n_ch1], stddev=0.1)),\n",
      "#     \"b2\" : tf.Variable(tf.random_normal([n_ch2], stddev=0.1)),\n",
      "#     \"b3\" : tf.Variable(tf.random_normal([n_ch3], stddev=0.1)),\n",
      "#     \"b4\" : tf.Variable(tf.random_normal([n_out], stddev=0.1))}\n",
      "\n",
      "# x = tf.placeholder(tf.float32, [None, 256, 256, 3])\n",
      "# y = tf.placeholder(tf.uint8, [None])\n",
      "# y_hot = tf.one_hot(y, n_out)\n",
      "\n",
      "# with tf.name_scope(\"Crop\") :\n",
      "#     crop = tf.map_fn(lambda img: tf.random_crop(img, [224, 224, 3]), x)\n",
      "\n",
      "# with tf.name_scope(\"Conv1\") :\n",
      "#     conv1 = tf.nn.conv2d(crop, W[\"w1\"], strides=[1,1,1,1], padding='SAME')\n",
      "#     mean, var = tf.nn.moments(conv1, [0,1,2])\n",
      "#     conv1 = tf.nn.batch_normalization(conv1, mean, var, 0, 1, 0.001)\n",
      "#     conv1 = tf.nn.relu(tf.nn.bias_add(conv1, B[\"b1\"]))\n",
      "#     pool1 = tf.nn.max_pool(conv1, ksize=[1,4,4,1], strides=[1,4,4,1], padding='SAME')\n",
      "\n",
      "# with tf.name_scope(\"Conv2\") :\n",
      "#     conv2 = tf.nn.conv2d(pool1, W[\"w2\"], strides=[1,1,1,1], padding='SAME')\n",
      "#     mean, var = tf.nn.moments(conv2, [0,1,2])\n",
      "#     conv2 = tf.nn.batch_normalization(conv2, mean, var, 0, 1, 0.001)\n",
      "#     conv2 = tf.nn.relu(tf.nn.bias_add(conv2, B[\"b2\"]))\n",
      "#     pool2 = tf.nn.max_pool(conv2, ksize=[1,4,4,1], strides=[1,4,4,1], padding='SAME')\n",
      "\n",
      "# with tf.name_scope(\"FC1\") :\n",
      "#     fc1 = tf.reshape(pool2, [-1, W[\"w3\"].get_shape().as_list()[0]])\n",
      "#     fc1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(fc1, W[\"w3\"]), B[\"b3\"]))\n",
      "\n",
      "# with tf.name_scope(\"FC2\") :\n",
      "#     fc2 = tf.nn.bias_add(tf.matmul(fc1, W[\"w4\"]), B[\"b4\"])\n",
      "\n",
      "# with tf.name_scope(\"Cost\") :\n",
      "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=fc2, labels=y_hot))\n",
      "# #     loss = loss + 0.0003*tf.add_n([ tf.nn.l2_loss(w) for w in W.values() ]) # L2 regularization\n",
      "#     opt = tf.train.AdamOptimizer(learning_rate=0.003).minimize(loss)\n",
      "\n",
      "# with tf.name_scope(\"Accuracy\") :\n",
      "#     corPred = tf.equal(tf.argmax(fc2, axis=1), tf.argmax(y_hot, axis=1))\n",
      "#     acc = tf.reduce_mean(tf.cast(corPred, tf.float32))\n",
      "    \n",
      "# print \"Structure build Complete\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # 5 layer\n",
      "\n",
      "# n_out = np.max(np.unique(y_data)) + 1\n",
      "# n_in = X_data.shape[1]*X_data.shape[2]\n",
      "# n_c1 = 8\n",
      "# n_c2 = 16\n",
      "# n_c3 = 32\n",
      "# n_fc1 = 128\n",
      "# W = {\"c1_1\" : tf.Variable(tf.truncated_normal([3, 3, 3, n_c1], stddev=0.1)),\n",
      "#     \"c1_2\" : tf.Variable(tf.truncated_normal([3, 3, n_c1, n_c1], stddev=0.1)),\n",
      "#     \"c2_1\" : tf.Variable(tf.truncated_normal([3, 3, n_c1, n_c2], stddev=0.1)),\n",
      "#     \"c2_2\" : tf.Variable(tf.truncated_normal([3, 3, n_c2, n_c2], stddev=0.1)),\n",
      "#     \"c3_1\" : tf.Variable(tf.truncated_normal([3, 3, n_c2, n_c3], stddev=0.1)),\n",
      "#     \"c3_2\" : tf.Variable(tf.truncated_normal([3, 3, n_c3, n_c3], stddev=0.1)),\n",
      "#     \"fc1\" : tf.Variable(tf.truncated_normal([8*8*n_c3, n_fc1], stddev=0.1)),\n",
      "#     \"fc2\" : tf.Variable(tf.truncated_normal([n_fc1, n_out], stddev=0.1))}\n",
      "# B = {\"c1_1\" : tf.Variable(tf.random_normal([n_c1], stddev=0.1)),\n",
      "#     \"c1_2\" : tf.Variable(tf.random_normal([n_c1], stddev=0.1)),\n",
      "#     \"c2_1\" : tf.Variable(tf.random_normal([n_c2], stddev=0.1)),\n",
      "#     \"c2_2\" : tf.Variable(tf.random_normal([n_c2], stddev=0.1)),\n",
      "#     \"c3_1\" : tf.Variable(tf.random_normal([n_c3], stddev=0.1)),\n",
      "#     \"c3_2\" : tf.Variable(tf.random_normal([n_c3], stddev=0.1)),\n",
      "#     \"fc1\" : tf.Variable(tf.random_normal([n_fc1], stddev=0.1)),\n",
      "#     \"fc2\" : tf.Variable(tf.random_normal([n_out], stddev=0.1))}\n",
      "\n",
      "# x = tf.placeholder(tf.float32, [None, 256, 256, 3])\n",
      "# y = tf.placeholder(tf.uint8, [None])\n",
      "# y_hot = tf.one_hot(y, n_out)\n",
      "\n",
      "# pool1 = conv33pool4(x, W[\"c1_1\"], B[\"c1_1\"], W[\"c1_2\"], B[\"c1_2\"], \"Conv1\")\n",
      "# pool2 = conv33pool4(pool1, W[\"c2_1\"], B[\"c2_1\"], W[\"c2_2\"], B[\"c2_2\"], \"Conv2\")\n",
      "# pool3 = conv33pool2(pool2, W[\"c3_1\"], B[\"c3_1\"], W[\"c3_2\"], B[\"c3_2\"], \"Conv3\")\n",
      "\n",
      "# with tf.name_scope(\"FC1\") :\n",
      "#     fc1 = tf.reshape(pool3, [-1, W[\"fc1\"].get_shape().as_list()[0]])\n",
      "#     fc1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(fc1, W[\"fc1\"]), B[\"fc1\"]))\n",
      "\n",
      "# with tf.name_scope(\"FC2\") :\n",
      "#     fc2 = tf.nn.bias_add(tf.matmul(fc1, W[\"fc2\"]), B[\"fc2\"])\n",
      "\n",
      "# with tf.name_scope(\"Cost\") :\n",
      "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=fc2, labels=y_hot))\n",
      "#     loss = loss + 0.1*tf.add_n([ tf.nn.l2_loss(w) for w in W.values() ]) # L2 regularization\n",
      "#     opt = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
      "\n",
      "# with tf.name_scope(\"Accuracy\") :\n",
      "#     corPred = tf.equal(tf.argmax(fc2, axis=1), tf.argmax(y_hot, axis=1))\n",
      "#     acc = tf.reduce_mean(tf.cast(corPred, tf.float32))\n",
      "    \n",
      "# print \"Structure build Complete\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # 7 layer\n",
      "\n",
      "# n_out = np.max(np.unique(y_data)) + 1\n",
      "# n_in = X_data.shape[1]*X_data.shape[2]\n",
      "# n_c1 = 8\n",
      "# n_c2 = 16\n",
      "# n_c3 = 16\n",
      "# n_c4 = 32\n",
      "# n_c5 = 64\n",
      "# n_fc1 = 128\n",
      "# W = {\"c1_1\" : tf.Variable(tf.truncated_normal([3, 3, 3, n_c1], stddev=0.1)),\n",
      "#     \"c1_2\" : tf.Variable(tf.truncated_normal([3, 3, n_c1, n_c1], stddev=0.1)),\n",
      "#     \"c2_1\" : tf.Variable(tf.truncated_normal([3, 3, n_c1, n_c2], stddev=0.1)),\n",
      "#     \"c2_2\" : tf.Variable(tf.truncated_normal([3, 3, n_c2, n_c2], stddev=0.1)),\n",
      "#     \"c3_1\" : tf.Variable(tf.truncated_normal([3, 3, n_c2, n_c3], stddev=0.1)),\n",
      "#     \"c3_2\" : tf.Variable(tf.truncated_normal([3, 3, n_c3, n_c3], stddev=0.1)),\n",
      "#     \"c4_1\" : tf.Variable(tf.truncated_normal([3, 3, n_c3, n_c4], stddev=0.1)),\n",
      "#     \"c4_2\" : tf.Variable(tf.truncated_normal([3, 3, n_c4, n_c4], stddev=0.1)),\n",
      "#     \"c5_1\" : tf.Variable(tf.truncated_normal([3, 3, n_c4, n_c5], stddev=0.1)),\n",
      "#     \"c5_2\" : tf.Variable(tf.truncated_normal([3, 3, n_c5, n_c5], stddev=0.1)),\n",
      "#     \"fc1\" : tf.Variable(tf.truncated_normal([8*8*n_c5, n_fc1], stddev=0.1)),\n",
      "#     \"fc2\" : tf.Variable(tf.truncated_normal([n_fc1, n_out], stddev=0.1))}\n",
      "# B = {\"c1_1\" : tf.Variable(tf.random_normal([n_c1], stddev=0.1)),\n",
      "#     \"c1_2\" : tf.Variable(tf.random_normal([n_c1], stddev=0.1)),\n",
      "#     \"c2_1\" : tf.Variable(tf.random_normal([n_c2], stddev=0.1)),\n",
      "#     \"c2_2\" : tf.Variable(tf.random_normal([n_c2], stddev=0.1)),\n",
      "#     \"c3_1\" : tf.Variable(tf.random_normal([n_c3], stddev=0.1)),\n",
      "#     \"c3_2\" : tf.Variable(tf.random_normal([n_c3], stddev=0.1)),\n",
      "#     \"c4_1\" : tf.Variable(tf.random_normal([n_c4], stddev=0.1)),\n",
      "#     \"c4_2\" : tf.Variable(tf.random_normal([n_c4], stddev=0.1)),\n",
      "#     \"c5_1\" : tf.Variable(tf.random_normal([n_c5], stddev=0.1)),\n",
      "#     \"c5_2\" : tf.Variable(tf.random_normal([n_c5], stddev=0.1)),\n",
      "#     \"fc1\" : tf.Variable(tf.random_normal([n_fc1], stddev=0.1)),\n",
      "#     \"fc2\" : tf.Variable(tf.random_normal([n_out], stddev=0.1))}\n",
      "\n",
      "# x = tf.placeholder(tf.float32, [None, 256, 256, 3])\n",
      "# y = tf.placeholder(tf.uint8, [None])\n",
      "# y_hot = tf.one_hot(y, n_out)\n",
      "\n",
      "# pool1 = conv33pool2(x, W[\"c1_1\"], B[\"c1_1\"], W[\"c1_2\"], B[\"c1_2\"], \"Conv1\")\n",
      "# pool2 = conv33pool2(pool1, W[\"c2_1\"], B[\"c2_1\"], W[\"c2_2\"], B[\"c2_2\"], \"Conv2\")\n",
      "# pool3 = conv33pool2(pool2, W[\"c3_1\"], B[\"c3_1\"], W[\"c3_2\"], B[\"c3_2\"], \"Conv3\")\n",
      "# pool4 = conv333pool2(pool3, W[\"c4_1\"], B[\"c4_1\"], W[\"c4_2\"], B[\"c4_2\"], \"Conv4\")\n",
      "# pool5 = conv333pool2(pool4, W[\"c5_1\"], B[\"c5_1\"], W[\"c5_2\"], B[\"c5_2\"], \"Conv5\")\n",
      "\n",
      "# with tf.name_scope(\"FC1\") :\n",
      "#     fc1 = tf.reshape(pool5, [-1, W[\"fc1\"].get_shape().as_list()[0]])\n",
      "#     fc1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(fc1, W[\"fc1\"]), B[\"fc1\"]))\n",
      "\n",
      "# with tf.name_scope(\"FC2\") :\n",
      "#     fc2 = tf.nn.bias_add(tf.matmul(fc1, W[\"fc2\"]), B[\"fc2\"])\n",
      "\n",
      "# with tf.name_scope(\"Cost\") :\n",
      "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=fc2, labels=y_hot))\n",
      "#     loss = loss + 0.01*tf.add_n([ tf.nn.l2_loss(w) for w in W.values() ]) # L2 regularization\n",
      "#     opt = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
      "\n",
      "# with tf.name_scope(\"Accuracy\") :\n",
      "#     corPred = tf.equal(tf.argmax(fc2, axis=1), tf.argmax(y_hot, axis=1))\n",
      "#     acc = tf.reduce_mean(tf.cast(corPred, tf.float32))\n",
      "    \n",
      "# print \"Structure build Complete\""
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Patching and 5layers\n",
      "\n",
      "do_l2 = V.get('do_l2')\n",
      "learning_rate = V.get('learning_rate')\n",
      "l2_rate = V.get('l2_rate')\n",
      "n_c1 = 64\n",
      "n_c2 = 128\n",
      "n_c3 = 256\n",
      "n_fc1 = 1024\n",
      "n_output = V.get('n_output')\n",
      "\n",
      "with tf.name_scope(\"Weight\") :\n",
      "    W = {\"c1_1\" : tf.Variable(tf.truncated_normal([3, 3, 3, n_c1], stddev=0.1), name=\"w_c1_1\"),\n",
      "        \"c1_2\" : tf.Variable(tf.truncated_normal([3, 3, n_c1, n_c1], stddev=0.1), name=\"w_c1_2\"),\n",
      "        \"c2_1\" : tf.Variable(tf.truncated_normal([3, 3, n_c1, n_c2], stddev=0.1), name=\"w_c2_1\"),\n",
      "        \"c2_2\" : tf.Variable(tf.truncated_normal([3, 3, n_c2, n_c2], stddev=0.1), name=\"w_c2_2\"),\n",
      "        \"c3_1\" : tf.Variable(tf.truncated_normal([3, 3, n_c2, n_c3], stddev=0.1), name=\"w_c3_1\"),\n",
      "        \"c3_2\" : tf.Variable(tf.truncated_normal([3, 3, n_c3, n_c3], stddev=0.1), name=\"w_c3_2\"),\n",
      "        \"fc1\" : tf.Variable(tf.truncated_normal([7*7*n_c3, n_fc1], stddev=0.1), name=\"w_fc1\"),\n",
      "        \"out\" : tf.Variable(tf.truncated_normal([n_fc1, n_output], stddev=0.1), name=\"w_out\")}\n",
      "    \n",
      "with tf.name_scope(\"Bias\") :\n",
      "    B = {\"c1_1\" : tf.Variable(tf.random_normal([n_c1], stddev=0.1), name=\"b_c1_1\"),\n",
      "        \"c1_2\" : tf.Variable(tf.random_normal([n_c1], stddev=0.1), name=\"b_c1_2\"),\n",
      "        \"c2_1\" : tf.Variable(tf.random_normal([n_c2], stddev=0.1), name=\"b_c2_1\"),\n",
      "        \"c2_2\" : tf.Variable(tf.random_normal([n_c2], stddev=0.1), name=\"b_c2_2\"),\n",
      "        \"c3_1\" : tf.Variable(tf.random_normal([n_c3], stddev=0.1), name=\"b_c3_1\"),\n",
      "        \"c3_2\" : tf.Variable(tf.random_normal([n_c3], stddev=0.1), name=\"b_c3_2\"),\n",
      "        \"fc1\" : tf.Variable(tf.random_normal([n_fc1], stddev=0.1), name=\"b_fc1\"),\n",
      "        \"out\" : tf.Variable(tf.random_normal([n_output], stddev=0.1), name=\"b_out\")}\n",
      "    \n",
      "with tf.name_scope(\"CROP\") :\n",
      "    crop = tf.map_fn(lambda img: tf.random_crop(img, [224, 224, 3]), x_std)\n",
      "\n",
      "pool1 = conv33pool4(crop, W[\"c1_1\"], B[\"c1_1\"], W[\"c1_2\"], B[\"c1_2\"], phase, \"CONV1\")\n",
      "pool2 = conv33pool4(pool1, W[\"c2_1\"], B[\"c2_1\"], W[\"c2_2\"], B[\"c2_2\"], phase, \"CONV2\")\n",
      "pool3 = conv33pool2(pool2, W[\"c3_1\"], B[\"c3_1\"], W[\"c3_2\"], B[\"c3_2\"], phase, \"CONV3\")\n",
      "\n",
      "with tf.name_scope(\"FC1\") :\n",
      "    fc1 = tf.reshape(pool3, [-1, W[\"fc1\"].get_shape().as_list()[0]])\n",
      "    fc1 = tf.nn.bias_add(tf.matmul(fc1, W[\"fc1\"]), B[\"fc1\"])\n",
      "    fc1 = tf.layers.batch_normalization(fc1, axis=1, center=True, scale=True, training=phase)\n",
      "    fc1 = tf.nn.relu(fc1)\n",
      "\n",
      "with tf.name_scope(\"OUTPUT\") :\n",
      "    fc2 = tf.nn.bias_add(tf.matmul(fc1, W[\"out\"]), B[\"out\"])\n",
      "\n",
      "with tf.name_scope(\"COST\") :\n",
      "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=fc2, labels=y_hot))\n",
      "    if do_l2 :\n",
      "        loss = loss + l2_rate*tf.add_n([ tf.nn.l2_loss(w) for w in W.values() ]) # L2 regularization\n",
      "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
      "    with tf.control_dependencies(extra_update_ops) :\n",
      "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
      "\n",
      "with tf.name_scope(\"Accuracy\") :\n",
      "    corPred = tf.equal(tf.argmax(fc2, 1), tf.argmax(y_hot, 1))\n",
      "    acc = tf.reduce_mean(tf.cast(corPred, tf.float32))\n",
      "    \n",
      "print \"Structure build Complete\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "TensorBoard"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)))\n",
      "print \"Session Created\"\n",
      "\n",
      "boardDir = \"/notebooks/board/SD/\"\n",
      "expTag = \"SimpleCNN\"\n",
      "expName = \"RotFlp_Crop_Layer5_x4_e003_1\"\n",
      "summary_writer = tf.summary.FileWriter(\"{:}{:}/{:}/\".format(boardDir, expTag, expName), sess.graph)\n",
      "V.put('model_save_path', \"./model/{:}/{:}/save\".format(expTag, expName))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.system(\"rm -r -f {}\".format(boardDir + expTag + expName))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Run"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "coord = tf.train.Coordinator()\n",
      "threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
      "\n",
      "model_save_path = V.get('model_save_path')\n",
      "training_iters = V.get('training_iters')\n",
      "display_step = V.get('display_step')\n",
      "save_step = V.get('save_step')\n",
      "batch_size = V.get('batch_size')\n",
      "train_batch_num = train_set_size/batch_size\n",
      "test_batch_num = test_set_size/batch_size\n",
      "\n",
      "sess.run(tf.global_variables_initializer())\n",
      "sess.run(tf.local_variables_initializer())\n",
      "\n",
      "saver = tf.train.Saver()\n",
      "\n",
      "print batch_size, train_batch_num, test_batch_num\n",
      "\n",
      "print \"Training...\"\n",
      "for iteration in range(training_iters) :\n",
      "    x_batch, y_batch = sess.run([train_image_batch, train_label_batch])\n",
      "    sess.run(opt, feed_dict={x:x_batch, y:y_batch, phase:True})\n",
      "    \n",
      "    if (iteration+1) % save_step == 0 :\n",
      "        saver.save(sess, model_save_path, global_step=iteration+1)\n",
      "        \n",
      "    if (iteration+1) % display_step == 0 :\n",
      "        summary = tf.Summary()\n",
      "        \n",
      "        train_cost, train_acc = sess.run([loss, acc], feed_dict={x:x_batch, y:y_batch, phase:False})#/train_batch_num\n",
      "#         train_acc = sess.run(acc, feed_dict={x:x_batch, y:y_batch})#/train_batch_num\n",
      "        summary.value.add(tag=\"train_cost\", simple_value=train_cost)\n",
      "        summary.value.add(tag=\"train_accuracy\", simple_value=train_acc)\n",
      "\n",
      "        test_cost = 0\n",
      "        test_acc = 0\n",
      "        for i in range(test_batch_num) :\n",
      "            x_batch, y_batch = sess.run([test_image_batch, test_label_batch])\n",
      "            test_cost_temp, test_acc_temp = sess.run([loss, acc], feed_dict={x:x_batch, y:y_batch, phase:False})#/test_batch_num\n",
      "            test_cost += test_cost_temp*batch_size\n",
      "            test_acc += test_acc_temp*batch_size\n",
      "        test_cost = test_cost/test_set_size\n",
      "        test_acc = test_acc/test_set_size\n",
      "        summary.value.add(tag=\"test_cost\", simple_value=test_cost)\n",
      "        summary.value.add(tag=\"test_accuracy\", simple_value=test_acc)\n",
      "        summary_writer.add_summary(summary, iteration)\n",
      "        \n",
      "        print \"iter: {:03d}/{:03d} Cost : {:.3f} TrainAcc : {:.3f} TestAcc : {:.3f}\".format(iteration+1, training_iters, train_cost, train_acc, test_acc)\n",
      "\n",
      "coord.request_stop()\n",
      "coord.join(threads)"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# saver = tf.train.Saver()\n",
      "# batch_size = V.get('batch_size')\n",
      "# train_batch_num = train_set_size/batch_size\n",
      "# test_batch_num = test_set_size/batch_size\n",
      "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess :\n",
      "    sess.run(tf.global_variables_initializer())\n",
      "    sess.run(tf.local_variables_initializer())\n",
      "    \n",
      "    saver = tf.train.import_meta_graph(\"./model/SimpleCNN/RotFlp_Crop_Layer5_x4_e003_1/{}.meta\".format(\"save-87000\"))\n",
      "#     saver = tf.train.Saver()\n",
      "    saver.restore(sess, tf.train.latest_checkpoint(\"./model/SimpleCNN/RotFlp_Crop_Layer5_x4_e003_1/\"))\n",
      "#     saver.restore(sess, \"./model/baseline.ckpt-111354.meta\")\n",
      "    graph = tf.get_default_graph()\n",
      "    col_list = graph.get_all_collection_keys()\n",
      "    print col_list\n",
      "    ops = graph.get_operations()\n",
      "    for i in ops :\n",
      "        print i\n",
      "#     loss = graph.get_operation_by_name(\"\")\n",
      "#     print \"Model restored.\"\n",
      "#     coord = tf.train.Coordinator()\n",
      "#     threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
      "#     test_cost = 0\n",
      "#     test_acc = 0\n",
      "#     for i in range(test_batch_num) :\n",
      "#         x_batch, y_batch = sess.run([test_image_batch, test_label_batch])\n",
      "#         test_cost_temp, test_acc_temp = sess.run([loss, acc], feed_dict={x:x_batch, y:y_batch, phase:False})#/test_batch_num\n",
      "#         test_cost += test_cost_temp*batch_size\n",
      "#         test_acc += test_acc_temp*batch_size\n",
      "#     test_cost = test_cost/test_set_size\n",
      "#     test_acc = test_acc/test_set_size\n",
      "#     print test_cost, test_acc\n",
      "\n",
      "#     coord.request_stop()\n",
      "#     coord.join(threads)"
     ],
     "language": "python",
     "metadata": {
      "scrolled": false
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print graph"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "tf.reset_default_graph()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "sess.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    }
   ],
   "metadata": {}
  }
 ]
}